{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Generate 3D point cloud with keras_cv stable_diffusion and OpenAI point-e"
      ],
      "metadata": {
        "id": "ISJMwWF2JXlG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q2uLkEuhxmdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f732c25-6311-42cd-c1f0-fcbb49e347bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for keras-cv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 29.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 78.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 66.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 669 kB 33.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/keras-team/keras-cv\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "!pip install -q transformers\n",
        "!pip -q install --upgrade gdown\n",
        "!pip -q install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Uf4-AZxcusQ8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras_cv.models.stable_diffusion import StableDiffusion, StableDiffusionV2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import plotly\n",
        "import clip\n",
        "from transformers import TFAutoModel\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn7vCYLSP2t1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "333219a47f1b4a36a3c07605e857c1f1",
            "8d643ca8e2544b51a1eeeef183b9edf7",
            "0dc0268eae864214952b700bd0b33005",
            "4fbba6cfdbbf460cbd5b928ff2501294",
            "58e19f16c409414e8438cad2c620e90c",
            "81cffb76dc8349a68c0c454044cfe69d",
            "0616562ab66d4bb68e8bb4f4c36bf581",
            "7cea4c77e9c74e27b183ec6ad8a126f9",
            "7c0109b504084dc18ee75251d08300ab",
            "ce0889db84764bd798226709af32a823",
            "dce3614d127144f382c00761f5b0eb6c",
            "770bf304bdf74a85bdbf2b07c96286bc",
            "697d4ea85ef345cfae80a7ee5b1c54e7",
            "2b0ffc2c91ba4769bb87f28f64d5ae43",
            "3ea64d11b1e2451e8a54d39720fefa8d",
            "7c1e6fbcbd814dcfb47c399b8a0953f1",
            "0a94e15552c74db487365fe5bd3f3079",
            "bf6a0149a8f740e89819e518c76692cd",
            "70c5c87615cc499daeda1a05ff937f2a",
            "cebc2adc865c450091c7a2e2043af6f7",
            "209b006c9cd24f2f84cf444c0281a183",
            "74ca48ea622a47c892c0f300e6fd3629"
          ]
        },
        "outputId": "ae967629-067f-49b5-e581-ec90c6a1f32a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/4.52k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "333219a47f1b4a36a3c07605e857c1f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "770bf304bdf74a85bdbf2b07c96286bc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "clip_model = TFAutoModel.from_pretrained('openai/clip-vit-large-patch14')\n",
        "clip_vision_model = clip_model.layers[0].vision_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XngbsMfAPC5v"
      },
      "outputs": [],
      "source": [
        "keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE_M2MMTkHvH"
      },
      "outputs": [],
      "source": [
        "# Lets define model configurations\n",
        "config = {\n",
        "    \"base40M\": {\n",
        "        \"heads\": 8,\n",
        "        \"layers\": 12,\n",
        "        \"n_ctx\": 1024,\n",
        "        \"width\": 512,\n",
        "        \"cond_ctx\":256,\n",
        "        \"cond_width\":1024,\n",
        "        \"input_channels\":6,\n",
        "        \"output_channels\":12,\n",
        "        \"weights\":{\"origin\": \"https://huggingface.co/Jobayer/keras_point_e/resolve/main/point_diffusion1.h5\",\n",
        "                   \"file_hash\": \"6385fe7c70793f74a99683b105a13a6f0cbc324bf93adb22f6f8cab824279d3b\"}\n",
        "    },\n",
        "    \"base300M\": {\n",
        "        \"heads\": 16,\n",
        "        \"layers\": 24,\n",
        "        \"n_ctx\": 1024,\n",
        "        \"width\": 1024,\n",
        "        \"cond_ctx\":256,\n",
        "        \"cond_width\":1024,\n",
        "        \"input_channels\":6,\n",
        "        \"output_channels\":12,\n",
        "        \"weights\":{\"origin\": \"https://huggingface.co/Jobayer/keras_point_e/resolve/main/point_diffusion_300M.h5\",\n",
        "                   \"file_hash\": \"84220fb81e504ebe65c0a4701f9643f1875f0e0f114539da766c426ce2bd9231\"}\n",
        "    },\n",
        "    \"upsample\": {\n",
        "        \"channel_biases\": [0.0, 0.0, 0.0, -1.0, -1.0, -1.0],\n",
        "        \"channel_scales\": [2.0, 2.0, 2.0, 0.007843137255, 0.007843137255, 0.007843137255],\n",
        "        \"cond_ctx\": 256,\n",
        "        \"cond_drop_prob\": 0.1,\n",
        "        \"heads\": 8,\n",
        "        \"init_scale\": 0.25,\n",
        "        \"input_channels\": 6,\n",
        "        \"layers\": 12,\n",
        "        \"n_ctx\": 3072,\n",
        "        \"width\": 512,\n",
        "        \"cond_width\": 1024,\n",
        "        \"low_res_ctx\": 1024,\n",
        "        \"input_channels\": 6,\n",
        "        \"output_channels\": 12,\n",
        "        \"weights\":{\"origin\": \"https://huggingface.co/Jobayer/keras_point_e/resolve/main/point_upsampler.h5\",\n",
        "                   \"file_hash\": \"004e909c76e8ea0fc987dfb7ae84eca22bb918f09fa2fa828ce6884ff8a1a93f\"}\n",
        "        \n",
        "    },\n",
        "    \"sdf\": {\n",
        "        \"decoder_heads\": 4,\n",
        "        \"decoder_layers\": 4,\n",
        "        \"encoder_heads\": 4,\n",
        "        \"encoder_layers\": 8,\n",
        "        \"init_scale\": 0.25,\n",
        "        \"n_ctx\": 4096,\n",
        "        \"name\": \"CrossAttentionPointCloudSDFModel\",\n",
        "        \"width\": 256,\n",
        "        \"weights\":{\"origin\": \"https://huggingface.co/Jobayer/keras_point_e/resolve/main/sdf.h5\",\n",
        "                   \"file_hash\": \"c0261aca2f0603cece98c981615682225c1e8ea2f1a1b99432ba0a328efa1e62\"}\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI64BJoZuhpE"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim=768, num_heads=12, causal=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.causal = causal\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.scale = ((self.head_dim)**-0.5)**0.5\n",
        "        self.in_proj = keras.layers.Dense(self.embed_dim*3)\n",
        "        self.out_proj = keras.layers.Dense(self.embed_dim)\n",
        "\n",
        "    def call(self, hidden_state):\n",
        "        _, tgt_length, embed_dim = hidden_state.shape\n",
        "        qkv = self.in_proj(hidden_state)\n",
        "        qkv = tf.reshape(qkv, [-1, tgt_length, self.num_heads, self.head_dim*3])\n",
        "        query, key, value = tf.split(qkv, 3, axis=-1)\n",
        "        attn_weights = tf.einsum(\"bthc,bshc->bhts\", query*self.scale, key*self.scale)\n",
        "        attn_weights = tf.nn.softmax(attn_weights)\n",
        "        attn_output = tf.einsum(\"bhts,bshc->bthc\", attn_weights, value)\n",
        "        attn_output = tf.reshape(attn_output, (-1, tgt_length, embed_dim))\n",
        "        return self.out_proj(attn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxNpcSuAfp_Q"
      },
      "outputs": [],
      "source": [
        "class MultiHeadCrossAttention(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim=768, num_heads=12, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.scale = ((self.head_dim)**-0.5)**0.5\n",
        "        self.query = keras.layers.Dense(self.embed_dim)\n",
        "        self.key_value = keras.layers.Dense(self.embed_dim*2)\n",
        "        self.out_proj = keras.layers.Dense(self.embed_dim)\n",
        "        \n",
        "    def call(self, hidden_state, encoded_hidden_states):\n",
        "        _, tgt_length, embed_dim = hidden_state.shape\n",
        "        query = self.query(hidden_state)\n",
        "        query = tf.reshape(query, [-1, tgt_length, self.num_heads, self.head_dim])\n",
        "        key_value = self.key_value(encoded_hidden_states)\n",
        "        key_value = tf.reshape(key_value, [-1, tgt_length, self.num_heads, self.head_dim*2])\n",
        "        key, value = tf.split(key_value, 2, axis=-1)\n",
        "        attn_weights = tf.einsum(\"bthc,bshc->bhts\", query*self.scale, key*self.scale)\n",
        "        attn_weights = tf.nn.softmax(attn_weights)\n",
        "        attn_output = tf.einsum(\"bhts,bshc->bthc\", attn_weights, value)\n",
        "        attn_output = tf.reshape(attn_output, (-1, tgt_length, embed_dim))\n",
        "        return self.out_proj(attn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg2VB9UtxITG"
      },
      "outputs": [],
      "source": [
        "class CLIPEncoderLayer(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_norm1 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads, causal=True)\n",
        "        self.layer_norm2 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.fc1 = keras.layers.Dense(embed_dim * 4)\n",
        "        self.fc2 = keras.layers.Dense(embed_dim)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        residual = inputs\n",
        "        x = self.layer_norm1(inputs)\n",
        "        x = self.attn(x)\n",
        "        x = residual + x\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.fc1(x)\n",
        "        x = tf.nn.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x + residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNu22R6RkIyS"
      },
      "outputs": [],
      "source": [
        "class CLIPDecoderLayer(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_norm1 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.attn = MultiHeadCrossAttention(embed_dim, num_heads)\n",
        "        self.layer_norm2 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.layer_norm3 = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.fc1 = keras.layers.Dense(embed_dim * 4)\n",
        "        self.fc2 = keras.layers.Dense(embed_dim)\n",
        "        \n",
        "    def call(self, hidden_state, encoded_hidden_state):\n",
        "        encoded_hidden_state = self.layer_norm1(encoded_hidden_state)\n",
        "        residual = hidden_state\n",
        "        x = self.layer_norm2(hidden_state)\n",
        "        x = self.attn(x, encoded_hidden_state)\n",
        "        x = residual + x\n",
        "        residual = x\n",
        "        x = self.layer_norm3(x)\n",
        "        x = self.fc1(x)\n",
        "        x = tf.nn.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x + residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnVi12ySz0Ag"
      },
      "outputs": [],
      "source": [
        "class CLIPEmbedding(keras.layers.Layer):\n",
        "  def __init__(self, width=512, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.ln = keras.layers.LayerNormalization(epsilon=1e-5)\n",
        "    self.fc = keras.layers.Dense(width)\n",
        "  def call(self, x):\n",
        "    x = self.ln(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31MlikoCAYNt"
      },
      "outputs": [],
      "source": [
        "class CLIPImageGridPointDiffusionTransformer(keras.Model):\n",
        "\n",
        "  def __init__(self, config, img_ctx_length=256, name=None, download_weights=True):\n",
        "    input = keras.layers.Input(shape=(config['n_ctx'], config['input_channels']))\n",
        "    t_emb_input = keras.layers.Input(shape=(config['width']))\n",
        "    cond_input = keras.layers.Input(shape=(config['cond_ctx'], config['cond_width']))\n",
        "\n",
        "    t_emb = keras.layers.Dense(config['width']*4)(t_emb_input)\n",
        "    t_emb = tf.nn.gelu(t_emb)\n",
        "    t_emb = keras.layers.Dense(config['width'])(t_emb)\n",
        "    \n",
        "    x = keras.layers.Dense(config['width'])(input)\n",
        "    cond_emb = CLIPEmbedding(config['width'])(cond_input)\n",
        "    x = keras.layers.Concatenate(axis=1)([x, t_emb[:,None], cond_emb])\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "    for _ in range(config['layers']):\n",
        "      x = CLIPEncoderLayer(config['width'], config['heads'])(x)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-5)(x) \n",
        "    output = keras.layers.Dense(config['output_channels'])(x[:, :config['n_ctx']])\n",
        "    super().__init__([input, t_emb_input, cond_input], output, name=name)\n",
        "    if download_weights:\n",
        "        weights_fpath = keras.utils.get_file(\n",
        "            origin=config[\"weights\"][\"origin\"],\n",
        "            file_hash=config[\"weights\"][\"file_hash\"],\n",
        "        )\n",
        "        self.load_weights(weights_fpath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6R81582IhaG"
      },
      "outputs": [],
      "source": [
        "class CLIPImageGridPointUpsamplerTransformer(keras.Model):\n",
        "\n",
        "  def __init__(self, config, img_ctx_length=256, name=None, download_weights=True):\n",
        "    input = keras.layers.Input(shape=(config['n_ctx'], config['input_channels']))\n",
        "    t_emb_input = keras.layers.Input(shape=(config['width']))\n",
        "    cond_input = keras.layers.Input(shape=(config['cond_ctx'], config['cond_width']))\n",
        "    low_res_input = keras.layers.Input(shape=(config['low_res_ctx'], config['input_channels']))\n",
        "\n",
        "    t_emb = keras.layers.Dense(config['width']*4, name='f_c')(t_emb_input)\n",
        "    t_emb = tf.nn.gelu(t_emb)\n",
        "    t_emb = keras.layers.Dense(config['width'], name='fc')(t_emb)\n",
        "    \n",
        "    if config['channel_scales'] is not None:\n",
        "        low_res = low_res_input * tf.convert_to_tensor(config['channel_scales'])[None,None,:]\n",
        "    if config['channel_biases'] is not None:\n",
        "        low_res = low_res + tf.convert_to_tensor(config['channel_biases'])[None,None,:]\n",
        "    low_res = keras.layers.Dense(config['width'])(low_res_input)\n",
        "    \n",
        "    cond_emb = CLIPEmbedding(config['width'])(cond_input)\n",
        "\n",
        "    x = keras.layers.Dense(config['width'], name='ic')(input)\n",
        "    x = keras.layers.Concatenate(axis=1)([x, t_emb[:,None], cond_emb, low_res])\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "    for _ in range(config['layers']):\n",
        "      x = CLIPEncoderLayer(config['width'], config['heads'])(x)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-5)(x) \n",
        "    output = keras.layers.Dense(config['output_channels'])(x[:, :config['n_ctx']])\n",
        "    super().__init__([input, t_emb_input, cond_input, low_res_input], output, name=name)\n",
        "    if download_weights:\n",
        "        weights_fpath = keras.utils.get_file(\n",
        "            origin=config[\"weights\"][\"origin\"],\n",
        "            file_hash=config[\"weights\"][\"file_hash\"],\n",
        "        )\n",
        "        self.load_weights(weights_fpath)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsqzWG78jALZ"
      },
      "outputs": [],
      "source": [
        "class PointCloudSDFModel(keras.Model):\n",
        "  def __init__(self, config, name=None, download_weights=True):\n",
        "    input = keras.layers.Input(shape=(config['n_ctx'], 3))\n",
        "    point_cloud = keras.layers.Input(shape=(config['n_ctx'], 3))\n",
        "\n",
        "    encoded_pc = keras.layers.Dense(config['width'])(point_cloud)\n",
        "    for _ in range(config['encoder_layers']):\n",
        "       encoded_pc = CLIPEncoderLayer(config['width'], config['encoder_heads'])(encoded_pc)\n",
        "\n",
        "    x = keras.layers.Dense(config['width'])(input)\n",
        "    for _ in range(config['decoder_layers']):\n",
        "      x = CLIPDecoderLayer(config['width'], config['decoder_heads'])(x, encoded_pc)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "    x = keras.layers.Dense(1)(x)\n",
        "    output = tf.squeeze(x)\n",
        "    super().__init__([input, point_cloud], output, name=name)\n",
        "    if download_weights:\n",
        "        weights_fpath = keras.utils.get_file(\n",
        "            origin=config[\"weights\"][\"origin\"],\n",
        "            file_hash=config[\"weights\"][\"file_hash\"],\n",
        "        )\n",
        "        self.load_weights(weights_fpath)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
        "  \"\"\"\n",
        "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
        "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
        "\n",
        "    :param num_diffusion_timesteps: the number of betas to produce.\n",
        "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
        "                      produces the cumulative product of (1-beta) up to that\n",
        "                      part of the diffusion process.\n",
        "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
        "                     prevent singularities.\n",
        "    \"\"\"\n",
        "  t1 = tf.range(num_diffusion_timesteps)/num_diffusion_timesteps\n",
        "  t2 = (tf.range(num_diffusion_timesteps) + 1)/num_diffusion_timesteps\n",
        "  betas = tf.minimum(1 - alpha_bar(t2) / alpha_bar(t1), max_beta)\n",
        "  return betas"
      ],
      "metadata": {
        "id": "40fTjuoTuqxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8npEMKwz3LFG"
      },
      "outputs": [],
      "source": [
        "num_diffusion_timesteps = 1024\n",
        "num_steps=(25, 64, 64)\n",
        "batch_size=1\n",
        "unconditional_guidance_scale=3\n",
        "seed = 000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stable_diffusion = StableDiffusion()"
      ],
      "metadata": {
        "id": "3HBpUb9a486Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTo3D:\n",
        "\n",
        "    def __init__(self, stable_diffusion, vision_model, num_steps=(25,64,64)):\n",
        "        #text_encoder = sd._text_encoder(jit_compile=True)\n",
        "        #imge_diffusion_model = sd._diffusion_model(jit_compile=True)\n",
        "        self.sd = stable_diffusion\n",
        "        self.point_diffusion_model = CLIPImageGridPointDiffusionTransformer(config['base300M'])\n",
        "        self.point_upsample_model = CLIPImageGridPointUpsamplerTransformer(config['upsample'])\n",
        "        self.sdf_model = PointCloudSDFModel(config['sdf'])\n",
        "        self.clip_vision_model = vision_model\n",
        "        self.preprocessing = clip.load('ViT-L/14')[-1]\n",
        "        self.num_steps=num_steps\n",
        "    def image_to_point_cloud(self, img_emb, num_steps=250):\n",
        "        latent = tf.random.normal(shape=(1,1024,6))\n",
        "        unconditional_emb = tf.zeros_like(img_emb)\n",
        "        betas = betas_for_alpha_bar(1024,\n",
        "                            lambda t: tf.math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n",
        "        )\n",
        "        timesteps = tf.range(1, 1024, 1024 // num_steps)\n",
        "        alphas_cumprod = tf.math.cumprod(1.0 - betas)\n",
        "        alphas = [alphas_cumprod[t] for t in timesteps]\n",
        "        alphas_prev = [1.0] + alphas[:-1]\n",
        "        progbar = keras.utils.Progbar(len(timesteps))\n",
        "        iteration = 0\n",
        "        for index, timestep in list(enumerate(timesteps))[::-1]:\n",
        "            latent_prev = tf.cast(latent, tf.float32)  # Set aside the previous latent vector\n",
        "            t_emb = sd._get_timestep_embedding(timestep, batch_size, dim=config['base300M']['width'])\n",
        "            latent = self.point_diffusion_model([\n",
        "                tf.concat([latent, latent],axis=0),\n",
        "                tf.concat([t_emb, t_emb],axis=0),\n",
        "                tf.concat([img_emb, unconditional_emb],axis=0)])\n",
        "            latent, rest = latent[...,:3], latent[...,3:6]\n",
        "            \n",
        "            latent, unconditional_latent = tf.split(latent, 2, axis=0)\n",
        "            latent = unconditional_latent + unconditional_guidance_scale * (\n",
        "                latent - unconditional_latent\n",
        "                )\n",
        "            latent = tf.concat([latent, rest[:batch_size]], axis=-1)\n",
        "            latent = tf.cast(latent, tf.float32)\n",
        "            \n",
        "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
        "            pred_x0 = (latent_prev - math.sqrt(1 - a_t) * latent[...,:6]) / math.sqrt(a_t)\n",
        "            latent = latent * math.sqrt(1.0 - a_prev) + math.sqrt(a_prev) * pred_x0\n",
        "            iteration += 1\n",
        "            progbar.update(iteration)\n",
        "        return latent\n",
        "\n",
        "    def upsample_point_cloud(self, img_emb, low_res, num_steps=250):\n",
        "        latent = tf.random.normal(shape=(1,3072,6))\n",
        "\n",
        "        unconditional_emb = tf.zeros_like(img_emb)\n",
        "        unconditional_low_res = tf.zeros_like(low_res)\n",
        "\n",
        "        timesteps = tf.range(1, 1024, 1024 // num_steps)\n",
        "        betas = tf.linspace((1000/1024) * 0.0001,  (1000/1024) * 0.02, 1024)\n",
        "        alphas_cumprod = tf.math.cumprod(1.0 - betas)\n",
        "        alphas = [alphas_cumprod[t] for t in timesteps]\n",
        "        alphas_prev = [1.0] + alphas[:-1]\n",
        "        \n",
        "        progbar = keras.utils.Progbar(len(timesteps))\n",
        "        iteration = 0\n",
        "        for index, timestep in list(enumerate(timesteps))[::-1]:\n",
        "            latent_prev = tf.cast(latent, tf.float32)  # Set aside the previous latent vector\n",
        "            t_emb = sd._get_timestep_embedding(timestep, batch_size, dim=config['upsample']['width'])\n",
        "            latent = self.point_upsample_model([\n",
        "                tf.concat([latent, latent],axis=0),\n",
        "                tf.concat([t_emb, t_emb],axis=0),\n",
        "                tf.concat([img_emb, unconditional_emb],axis=0),\n",
        "                tf.concat([low_res, unconditional_low_res], axis=0)]\n",
        "                )\n",
        "            latent, rest = latent[...,:3], latent[...,3:6]\n",
        "            \n",
        "            latent, unconditional_latent = tf.split(latent, 2, axis=0)\n",
        "            latent = unconditional_latent + unconditional_guidance_scale * (\n",
        "                latent - unconditional_latent\n",
        "                )\n",
        "            latent = tf.concat([latent, rest[:batch_size]], axis=-1)\n",
        "            latent = tf.cast(latent, tf.float32)\n",
        "            \n",
        "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
        "            pred_x0 = (latent_prev - math.sqrt(1 - a_t) * latent[...,:6]) / math.sqrt(a_t)\n",
        "            latent = latent * math.sqrt(1.0 - a_prev) + math.sqrt(a_prev) * pred_x0\n",
        "            iteration += 1\n",
        "            progbar.update(iteration)\n",
        "        return latent\n",
        "\n",
        "    def clip_image_encoder(self, image):\n",
        "        image = Image.fromarray(image[0])\n",
        "        image = self.preprocessing(image)\n",
        "        embd = self.clip_vision_model.embeddings(tf.convert_to_tensor(image)[None,])\n",
        "        embd = self.clip_vision_model.pre_layernorm(embd)\n",
        "        img_emb = self.clip_vision_model.encoder(embd,\n",
        "                     attention_mask=None,\n",
        "                     causal_attention_mask=True,\n",
        "                     output_attentions=False,\n",
        "                     output_hidden_states=False,\n",
        "                     return_dict=False)[0][:,1:]\n",
        "        return img_emb\n",
        "    def text_to_3d(self, prompt='A 3D avater of a dog'):\n",
        "        image = self.sd.text_to_image(prompt=prompt, num_steps=self.num_steps[0], seed=seed)\n",
        "        img_emb = self.clip_image_encoder(image)\n",
        "        pc = self.image_to_point_cloud(img_emb, self.num_steps[1])\n",
        "        upsampled_pc = self.upsample_point_cloud(img_emb, pc, self.num_steps[2])\n",
        "        final_pc = tf.concat([pc, upsampled_pc], axis=1)\n",
        "        return {\"image\": image,\n",
        "                \"pc\":final_pc\n",
        "        }\n",
        "   \n",
        "    def plot(self, latent):\n",
        "        return plotly.graph_objects.Figure(\n",
        "            data=[\n",
        "                plotly.graph_objects.Scatter3d(\n",
        "                    x=latent[:,0], y=latent[:,1], z=latent[:,2], \n",
        "                    mode='markers',\n",
        "                    marker=dict(\n",
        "                      size=2,\n",
        "                      color=['rgb({},{},{})'.format(r,g,b) for r,g,b in zip(np.clip(latent[:,3],0.,1.)*255.,\n",
        "                                                                            np.clip(latent[:,4], 0.,1.)*255.,\n",
        "                                                                            np.clip(latent[:,5], 0.,1.)*255.)],\n",
        "                                )\n",
        "                    )\n",
        "                ],\n",
        "                layout=dict(\n",
        "                    scene=dict(xaxis=dict(visible=False), yaxis=dict(visible=False), zaxis=dict(visible=False))\n",
        "                    ),\n",
        "                    )"
      ],
      "metadata": {
        "id": "-jjzTNBt12Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_3d = TextTo3D(stable_diffusion, clip_vision_model, num_steps=num_steps)"
      ],
      "metadata": {
        "id": "isqLsvCuTzIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = text_to_3d.text_to_3d('A high quality 3D render of a dog. Left view')"
      ],
      "metadata": {
        "id": "G85xhiI1WNO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(image):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "eq9q0f9m5qNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_image(sample['image'][0])"
      ],
      "metadata": {
        "id": "KeVij_M16Ljp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_3d.plot(sample['pc'][0])"
      ],
      "metadata": {
        "id": "URN02q655RQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_RINZ9Uv_AS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.9 64-bit ('3.9.9')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "b270b0f43bc427bcab7703c037711644cc480aac7c1cc8d2940cfaf0b447ee2e"
      }
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "333219a47f1b4a36a3c07605e857c1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d643ca8e2544b51a1eeeef183b9edf7",
              "IPY_MODEL_0dc0268eae864214952b700bd0b33005",
              "IPY_MODEL_4fbba6cfdbbf460cbd5b928ff2501294"
            ],
            "layout": "IPY_MODEL_58e19f16c409414e8438cad2c620e90c"
          }
        },
        "8d643ca8e2544b51a1eeeef183b9edf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81cffb76dc8349a68c0c454044cfe69d",
            "placeholder": "​",
            "style": "IPY_MODEL_0616562ab66d4bb68e8bb4f4c36bf581",
            "value": "Downloading: 100%"
          }
        },
        "0dc0268eae864214952b700bd0b33005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cea4c77e9c74e27b183ec6ad8a126f9",
            "max": 4519,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c0109b504084dc18ee75251d08300ab",
            "value": 4519
          }
        },
        "4fbba6cfdbbf460cbd5b928ff2501294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce0889db84764bd798226709af32a823",
            "placeholder": "​",
            "style": "IPY_MODEL_dce3614d127144f382c00761f5b0eb6c",
            "value": " 4.52k/4.52k [00:00&lt;00:00, 216kB/s]"
          }
        },
        "58e19f16c409414e8438cad2c620e90c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81cffb76dc8349a68c0c454044cfe69d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0616562ab66d4bb68e8bb4f4c36bf581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cea4c77e9c74e27b183ec6ad8a126f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c0109b504084dc18ee75251d08300ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce0889db84764bd798226709af32a823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce3614d127144f382c00761f5b0eb6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "770bf304bdf74a85bdbf2b07c96286bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_697d4ea85ef345cfae80a7ee5b1c54e7",
              "IPY_MODEL_2b0ffc2c91ba4769bb87f28f64d5ae43",
              "IPY_MODEL_3ea64d11b1e2451e8a54d39720fefa8d"
            ],
            "layout": "IPY_MODEL_7c1e6fbcbd814dcfb47c399b8a0953f1"
          }
        },
        "697d4ea85ef345cfae80a7ee5b1c54e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a94e15552c74db487365fe5bd3f3079",
            "placeholder": "​",
            "style": "IPY_MODEL_bf6a0149a8f740e89819e518c76692cd",
            "value": "Downloading:  76%"
          }
        },
        "2b0ffc2c91ba4769bb87f28f64d5ae43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c5c87615cc499daeda1a05ff937f2a",
            "max": 1711114176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cebc2adc865c450091c7a2e2043af6f7",
            "value": 1292474368
          }
        },
        "3ea64d11b1e2451e8a54d39720fefa8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_209b006c9cd24f2f84cf444c0281a183",
            "placeholder": "​",
            "style": "IPY_MODEL_74ca48ea622a47c892c0f300e6fd3629",
            "value": " 1.29G/1.71G [00:29&lt;00:10, 39.8MB/s]"
          }
        },
        "7c1e6fbcbd814dcfb47c399b8a0953f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a94e15552c74db487365fe5bd3f3079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6a0149a8f740e89819e518c76692cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70c5c87615cc499daeda1a05ff937f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cebc2adc865c450091c7a2e2043af6f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "209b006c9cd24f2f84cf444c0281a183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74ca48ea622a47c892c0f300e6fd3629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}